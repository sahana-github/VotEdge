[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /path/to/airflow

# The folder where your airflow pipelines live, most likely a
# subfolder in a code repository
dags_folder = ./dags

# Hostname by providing a path to a callable that returns a string
# Each DagRun's execution_date is compared to the clock time to determine whether the
# DagRun is expired. To prevent very old DagRuns from running, this setting defines the
# threshold for "expiration". The default is two weeks.
dag_dir_list_interval = 300

# The executor is used to coordinate the execution of task instances
executor = SequentialExecutor

# Parallelism affects the number of task instances that can run simultaneously
parallelism = 32

[logging]
# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search.
base_log_folder = ./logs
logging_level = INFO

[cli]
# In what way should the cli access the API. The original default of ``auth`` is recommended.
api_client = airflow.api.client.local_client
endpoint_url = http://localhost:8080

[api]
# How to authenticate users of the API
auth_backend = airflow.api.auth.backend.basic_auth

[webserver]
# The base url of your website as airflow cannot guess what domain or
# cname you are using. This is used in automated emails that
# airflow sends to mark "More Info".
base_url = http://localhost:8080

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# The time after which a DAG is marked as a failure
dag_default_view = tree